---
title: "Health Insurance Cross Sell - Part 03"
author: "Edneide Ramalho"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output: 
    html_document:
      highlight: textmate
      logo: logo.png
      theme: jou
      number_sections: yes
      toc: yes
      toc_float:
        collapsed: yes
        smooth_scroll: no
      df_print: paged
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Imports

```{r}
library(tidyverse)
library(janitor)
library(readr)
library(gtsummary)
library(summarytools)
library(kableExtra)
library(knitr)
library(gridExtra)
library(summarytools)
library(randomForest)
library(reshape2)
library(tidymodels)
```

# Helper Functions

```{r functions}
# Encoding
region_encoder <- readRDS("region_encoder.rds")
policy_encoder <- readRDS("policy_encoder.rds")

# Create function
encoder_function <- function(df){
  df %>% 
  left_join(region_encoder) %>% 
  select(-region_code) %>% 
  rename(region_code = region_num) %>% 
  left_join(policy_encoder) %>% 
  select(-policy_sales_channel) %>% 
  rename(policy_sales_channel = policy_num) 
}

# Top @K precision and recall function ---------
metrics_at_k_function <- function(model_results, k){
  df_results <- model_results %>% 
  arrange(desc(.pred_yes)) %>% 
  mutate(
    TP = ifelse(.pred_class == "yes" & response == "yes", 1, 0),
    FP = ifelse(.pred_class == "yes" & response == "no", 1, 0),
    FN = ifelse(.pred_class == "no" & response == "yes", 1, 0),
    TN = ifelse(.pred_class == "no" & response == "no", 1, 0)
  ) 
  
  # Create list for precision and recall
  precision_at_k <- list()
  recall_at_k <- list()

  # Populate the metric list
  for (i in 1:k) {
    subset_k <- df_results %>% 
    dplyr_row_slice(1:i)
    
    precision_at_k[[i]] <- (subset_k$TP %>% sum())/(subset_k$TP %>% sum() + subset_k$FP %>% sum())
  
    recall_at_k[[i]] <- (subset_k$TP %>% sum())/(subset_k$TP %>% sum() + subset_k$FN %>% sum())
}

    metrics_at_k <- df_results %>% 
  dplyr_row_slice(1:k) %>% 
  mutate(
    precision_at_k = unlist(precision_at_k),
    recall_at_k = unlist(recall_at_k)
  )
    
 return(metrics_at_k)
}

# Final metrics @K Function ----------------------
final_metrics_at_k <- function(model_results, k){
  
  model_metrics_at_k <- metrics_at_k_function(model_results, k)

  model_metrics_at_k %>% 
    slice(k) %>% 
    select(precision_at_k, recall_at_k)
}
```

# Data

```{r}
df <- readRDS("df_cleaned.rds")

selected_columns <- c(
  "id", 
  "age",
  "vehicle_damage",
  "days_associated",
  "previously_insured",
  "health_annual_paid", 
  "policy_sales_channel", 
  "region_code",
  "response"
)

# Final dataset
df_selected <- df %>% 
  select(all_of(selected_columns)) 

saveRDS(df_selected, "df_selected.rds")
```

# Pre-processing

```{r}
df_selected.rds <- readRDS("df_selected.rds")
df_selected <- encoder_function(df_selected)
```

# Splitting into train and test data sets

```{r}
set.seed(123)

df_split <- df_selected %>% 
  initial_split(prop = 0.80, strata = response)

df_train <- df_split %>% 
  training()

df_test <- df_split %>% 
  testing()
```

```{r}
df_recipe <- recipe(response ~ .,
       data = df_train %>% select(-id)) %>% 
  step_normalize(age, days_associated) %>% 
  step_scale(health_annual_paid) %>% 
  step_dummy(all_nominal(), -all_outcomes())
```

# Kfold cross validation

```{r}
df_kfolds <- vfold_cv(df_train %>% select(-id),
                      v = 5, strata = response)
```

# Logistic Regression ðŸ’»

```{r}
# Model specification
lr_model <- logistic_reg(
  penalty = tune(),
  mixture = tune()
) %>% 
  set_engine('glmnet') %>% 
  set_mode('classification')

hardhat::extract_parameter_set_dials(lr_model)
```

```{r}
# Creating the grid
lr_grid <- grid_regular(extract_parameter_set_dials(lr_model),
                        levels = 5)
```

```{r, eval = FALSE}
library(glmnet)

doParallel::registerDoParallel()

start_time <- Sys.time()

# Tune the model
lr_tune <- tune_grid(
  lr_model, df_recipe,
  resamples = df_kfolds,
  grid = lr_grid
)


end_time <- Sys.time()

print(end_time - start_time )

saveRDS(lr_tune, "lr_tune.rds")
```

```{r}
lr_tune <- readRDS("lr_tune.rds")

# Select the best hyperparameters
lr_param <- lr_tune %>% 
  select_best("roc_auc")

# Apply the best hyperparameters to the model
tidy_lr_model <- finalize_model(lr_model, lr_param)

# Create workflow
lr_wkfl <- workflow() %>% 
  add_model(tidy_lr_model) %>% 
  add_recipe(df_recipe)
```

```{r, eval=FALSE}
# Train final model
doParallel::registerDoParallel()

start_time <- Sys.time()

# Train the model
lr_res <- last_fit(lr_wkfl, df_split)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(lr_res, "lr_res.rds")
```

```{r}
lr_res <- readRDS("lr_res.rds")

# Confusion matrix
lr_res %>% 
  unnest(.predictions) %>% 
  conf_mat(truth = response, estimate = .pred_class)
  
```

```{r}
lr_results <- lr_res %>% 
  unnest(.predictions) %>% 
  select(.pred_yes:response)

# Metrics @K
lr_metrics_at_k <- final_metrics_at_k(lr_results, 2000)
lr_metrics_at_k <- tibble(model = "Logistic Regression") %>% 
  bind_cols(lr_metrics_at_k)
lr_metrics_at_k
```

```{r}
# Gain and Lift Curves
gain_curve(lr_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(lr_results, response, .pred_yes) %>% 
  autoplot()
```

Gain:By approaching 25% of the ordered list, \~ 61% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performs \~ 2.3 times better than the random list.

# Decision Tree ðŸ’»

```{r}
# Model specification ----------------
tree_model <- decision_tree(
  cost_complexity = tune(),
  min_n = tune(),
  tree_depth = tune()
) %>% 
  set_engine('rpart') %>% 
  set_mode('classification')

hardhat::extract_parameter_set_dials(tree_model)

# Creating the grid
tree_grid <- grid_regular(extract_parameter_set_dials(tree_model),
                        levels = 3)
```

```{r, eval = FALSE}
# Tune the model ------------
doParallel::registerDoParallel()
start_time <- Sys.time()

tree_tune <- tune_grid(
  tree_model, df_recipe,
  resamples = df_kfolds,
  grid = tree_grid
)

end_time <- Sys.time()

print(end_time - start_time )
saveRDS(tree_tune, "tree_tune.rds")
```

```{r}
tree_tune <- readRDS("tree_tune.rds")

# Select the best hyperparameters ---------------
tree_param <- tree_tune %>% 
  select_best("roc_auc")

# Apply the best hyperparameters to the model ----------
tidy_tree_model <- finalize_model(tree_model, tree_param)

# Create workflow ---------------
tree_wkfl <- workflow() %>% 
  add_model(tidy_tree_model) %>% 
  add_recipe(df_recipe)
```

```{r, eval=FALSE}
# Train final model -----------------
doParallel::registerDoParallel()
start_time <- Sys.time()

tree_res <- last_fit(tree_wkfl, df_split)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(tree_res, "tree_res.rds")
```

```{r}
# Confusion matrix -----------------
tree_res <- readRDS("tree_res.rds")

tree_conf_mat <- tree_res %>% 
  unnest(.predictions) %>% 
  conf_mat(truth = response, estimate = .pred_class)

# Results - data frame ---------
tree_results <- tree_res %>% 
  unnest(.predictions) %>% 
  select(.pred_yes:response)

# Metrics @K -------------------
tree_metrics_at_k <- final_metrics_at_k(tree_results, 2000)
tree_metrics_at_k <- tibble(model = "Decision Tree") %>% 
  bind_cols(tree_metrics_at_k)
tree_metrics_at_k

# Gain and Lift Curves --------------------
gain_curve(tree_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(tree_results, response, .pred_yes) %>% 
  autoplot()
```

Gain:By approaching 25% of the ordered list, \~ 62% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performs \~ 2.6 times better than the random list.

# Random Forest ðŸ’»

```{r}
# Model specification ----------------
rf_model <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 100
) %>% 
  set_engine('ranger') %>% 
  set_mode('classification')

hardhat::extract_parameter_set_dials(rf_model)

# Creating the grid
rf_grid <- grid_regular(
  mtry(range = c(10, 300)),
  min_n(range = c(100, 1000)),
  levels = 3
  )
```

```{r, eval = FALSE}
# Tune the model ------------
doParallel::registerDoParallel()
start_time <- Sys.time()

rf_tune <- tune_grid(
  rf_model, df_recipe,
  resamples = df_kfolds,
  grid = rf_grid
)

end_time <- Sys.time()

print(end_time - start_time )
saveRDS(rf_tune, "rf_tune.rds")
```

```{r}
rf_tune <- readRDS("rf_tune.rds")

# Select the best hyperparameters ---------------
rf_param <- rf_tune %>% 
  select_best("roc_auc")

# Apply the best hyperparameters to the model ----------
tidy_rf_model <- finalize_model(rf_model, rf_param)

# Create workflow ---------------
rf_wkfl <- workflow() %>% 
  add_model(tidy_rf_model) %>% 
  add_recipe(df_recipe)
```

```{r, eval=FALSE}
# Train final model -----------------
doParallel::registerDoParallel()
start_time <- Sys.time()

rf_res <- last_fit(rf_wkfl, df_split)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(rf_res, "rf_res.rds")
```

```{r}
# Confusion matrix -----------------
rf_res <- readRDS("rf_res.rds")

rf_conf_mat <- rf_res %>% 
  unnest(.predictions) %>% 
  conf_mat(truth = response, estimate = .pred_class)

# Results - data frame ---------
rf_results <- rf_res %>% 
  unnest(.predictions) %>% 
  select(.pred_yes:response)

# Metrics @K -------------------
rf_metrics_at_k <- final_metrics_at_k(rf_results, 2000)
rf_metrics_at_k <- tibble(model = "Random Forest") %>% 
  bind_cols(rf_metrics_at_k)
rf_metrics_at_k

# Gain and Lift Curves --------------------
gain_curve(rf_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(rf_results, response, .pred_yes) %>% 
  autoplot()
```

Gain:By approaching 25% of the ordered list, \~ 62% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performs \~ 2.7 times better than the random list.


# XGBoost ðŸ’»

```{r}
# Model specification ----------------
xgb_model <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) %>% 
  set_engine('xgboost') %>% 
  set_mode('classification')

hardhat::extract_parameter_set_dials(xgb_model)

# Creating the grid
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 10
  )
```

```{r, eval = FALSE}
# Tune the model ------------
doParallel::registerDoParallel()
start_time <- Sys.time()

xgb_tune <- tune_grid(
  xgb_model, df_recipe,
  resamples = df_kfolds,
  grid = xgb_grid
)

end_time <- Sys.time()

print(end_time - start_time )
saveRDS(xgb_tune, "xgb_tune.rds")
```

```{r}
xgb_tune <- readRDS("xgb_tune.rds")

# Select the best hyperparameters ---------------
xgb_param <- xgb_tune %>% 
  select_best("roc_auc")

# Apply the best hyperparameters to the model ----------
tidy_xgb_model <- finalize_model(xgb_model, xgb_param)

# Create workflow ---------------
xgb_wkfl <- workflow() %>% 
  add_model(tidy_xgb_model) %>% 
  add_recipe(df_recipe)
```

```{r, eval=FALSE}
# Train final model -----------------
doParallel::registerDoParallel()
start_time <- Sys.time()

xgb_res <- last_fit(xgb_wkfl, df_split)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(xgb_res, "xgb_res.rds")
```

```{r}
# Confusion matrix -----------------
xgb_res <- readRDS("xgb_res.rds")

xgb_conf_mat <- xgb_res %>% 
  unnest(.predictions) %>% 
  conf_mat(truth = response, estimate = .pred_class)

# Results - data frame ---------
xgb_results <- xgb_res %>% 
  unnest(.predictions) %>% 
  select(.pred_yes:response)

# Metrics @K -------------------
xgb_metrics_at_k <- final_metrics_at_k(xgb_results, 2000)
xgb_metrics_at_k <- tibble(model = "XGBoost") %>% 
  bind_cols(xgb_metrics_at_k)
xgb_metrics_at_k

# Gain and Lift Curves --------------------
gain_curve(xgb_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(xgb_results, response, .pred_yes) %>% 
  autoplot()
```

Gain:By approaching 25% of the ordered list, \~ 63% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performs \~ 2.8 times better than the random list.

# KNN ðŸ’»

```{r}
# Model specification ----------------
knn_model <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode('classification')

hardhat::extract_parameter_set_dials(knn_model)

# Creating the grid
knn_grid <- grid_regular(
  extract_parameter_set_dials(knn_model),
  levels = 10
)
```

```{r, eval = FALSE}
# Tune the model ------------
doParallel::registerDoParallel()
start_time <- Sys.time()

knn_tune <- tune_grid(
  knn_model, df_recipe,
  resamples = df_kfolds,
  grid = knn_grid
)

end_time <- Sys.time()

print(end_time - start_time )
saveRDS(knn_tune, "knn_tune.rds")
```

```{r}
knn_tune <- readRDS("knn_tune.rds")

# Select the best hyperparameters ---------------
knn_param <- knn_tune %>% 
  select_best("roc_auc")

# Apply the best hyperparameters to the model ----------
tidy_knn_model <- finalize_model(knn_model, knn_param)

# Create workflow ---------------
knn_wkfl <- workflow() %>% 
  add_model(tidy_knn_model) %>% 
  add_recipe(df_recipe)
```

```{r, eval=FALSE}
# Train final model -----------------
doParallel::registerDoParallel()
start_time <- Sys.time()

knn_res <- last_fit(knn_wkfl, df_split)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(knn_res, "knn_res.rds")
```

```{r}
# Confusion matrix -----------------
knn_res <- readRDS("knn_res.rds")

knn_conf_mat <- knn_res %>% 
  unnest(.predictions) %>% 
  conf_mat(truth = response, estimate = .pred_class)

# Results - data frame ---------
knn_results <- knn_res %>% 
  unnest(.predictions) %>% 
  select(.pred_yes:response)

# Metrics @K -------------------
knn_metrics_at_k <- final_metrics_at_k(knn_results, 2000)
knn_metrics_at_k <- tibble(model = "KNN") %>% 
  bind_cols(knn_metrics_at_k)
knn_metrics_at_k

# Gain and Lift Curves --------------------
gain_curve(knn_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(knn_results, response, .pred_yes) %>% 
  autoplot()
```

Gain:By approaching 25% of the ordered list, \~ 62.5% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performs \~ 2.5 times better than the random list.


# Model Comparison
```{r}
models_df <- bind_rows(
  lr_metrics_at_k,
  tree_metrics_at_k,
  rf_metrics_at_k,
  xgb_metrics_at_k,
  knn_metrics_at_k
) %>% 
  arrange(desc(recall_at_k))
models_df
```

The KNN model was the one that performed better in this cicle. So, it will be selected as our final model to make predictions for new clients. 

# Save final model

```{r, eval=FALSE}
start_time <- Sys.time()
doParallel::registerDoParallel()

final_model <- fit(knn_wkfl, df_selected)

end_time <- Sys.time()
print(end_time - start_time)

saveRDS(final_model, "final_model.rds")
```

```{r}
# Read the final model
model <- readRDS("final_model.rds")
```

## Make predictions 

```{r}
model$pre$mold$predictors %>% 
  colnames() %>% 
  tibble()
```

```{r}
clients <- tibble(
  "age" = 18,                  
  "days_associated" = 299,      
  "health_annual_paid" = 60000,   
  "region_code" = 28,           
  "policy_sales_channel" = 100, 
  "vehicle_damage" = "yes",     
  "previously_insured" = "no"
)

# Test for more than one client
clients2 <- df[1:100,]
selected <- c("id", names(clients))

clients2 <- clients2 %>% 
  select(one_of(selected))
```

```{r}
predict_one <- predict(model, clients, type = "prob")
predict_one
```

```{r}
predict_clients <- predict(model, 
                           clients2 %>% select(-id), 
                           type = "prob")

predict_clients <- clients2 %>% 
  select(id) %>% 
  bind_cols(predict_clients) %>% 
  arrange(desc(.pred_yes))
```






